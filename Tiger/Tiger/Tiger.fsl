{   
// header: any valid F# can appear here.   
open System
open Microsoft.FSharp.Text.Lexing   
open TigerAST

let comments =
    [
        "/*", TigerAST.CommentStart;
        "*/", TigerAST.CommentEnd;
    ] |> Map.ofList

let ops =   
    [   
        "=",    TigerAST.Equal;   
        "<",    TigerAST.Less;   
        "<=",   TigerAST.LessEqual;   
        ">",    TigerAST.Greater;   
        ">=",   TigerAST.GreaterEqual;
        ":=",   TigerAST.Assign;
        "+",    TigerAST.Plus;
        "-",    TigerAST.Minus;
        "/",    TigerAST.Divide; 
        "*",    TigerAST.Multiply;
        "**",   TigerAST.Exponent; 
    ] |> Map.ofList   

let keywords =   
    [   
        "int",     INT;
        "array",   ARRAY;
        "type",    TYPE;
        "if",      IF; 
        "then",    THEN;
        "else",    ELSE;
        "for",     FOR;
        "to",      TO;
        "do",      DO; 
        "while",   WHILE;
        "function",FUNCTION;
        "let",     LET; 
        "in",      IN;
        "nil",     NIL; 
    ] |> Map.ofList   
}   
 
// The let bindings at the top of the file are used to define 
// regular expression macros.
// regex macros   
let char        = ['a'-'z' 'A'-'Z']   
let digit       = ['0'-'9']   
let int        = '-'?digit+   
let float       = '-'?digit+ '.' digit+   
let whitespace  = [' ' '\t']   
let newline     = "\n\r" | '\n' | '\r'  
let comment     = "/*" | "*/"
let operator    = "=" | "<" | "<=" | ">" | ">=" | ":=" | "+" | "-" | "/" | "*" | "**"
//let keyword     = "INT" | "ARRAY" | "TYPE" | "IF" | "THEN"| "ELSE" | "FOR" | "TO" | "DO" | "WHILE" | "FUNCTION" | "LET" | "IN" | "NIL"
let identifier  = char(char|digit|['-' '_' '.'])*

// rules   
rule tokenize = parse   
// A series of rules, each of which have two pieces: 
// 1) a regular expression, 
// 2) an expression to evaluate if the regex matches, 
//    such as returning a token.
// Text is read from the token stream one character at 
// a time until it matches a regular expression and 
// returns a token.
| whitespace    { tokenize lexbuf }   
// Notice: 
//    - The code between the {'s and }'s consists of
//    plain old F# code. 
//    - We are returning the same tokens (INT, 
//    FLOAT, COMMA and EOF) that we defined in TigerParser.fsp
| newline       { lexbuf.EndPos <- lexbuf.EndPos.NextLine; tokenize lexbuf; }   
// 'lexeme lexbuf' returns the string our parser matched
| int           { Int(Int32.Parse(LexBuffer<_>.LexemeString lexbuf)) }   
| float         { Float(Double.Parse(LexBuffer<_>.LexemeString lexbuf)) }   
| ','           { TigerAST.Comma }   
| operator      { TigerAST.BinaryOperator(ops.[LexBuffer<_>.LexemeString lexbuf]) }  
| comment       { comments.[LexBuffer<_>.LexemeString lexbuf] }  
| identifier    { match keywords.TryFind(LexBuffer<_>.LexemeString lexbuf) with   
                  | Some(token) -> TigerAST.Keyword(token)
                  | None -> ID(LexBuffer<_>.LexemeString lexbuf) 
                } 
//{ TigerAST.Keyword(keywords.[LexBuffer<_>.LexemeString lexbuf])
// eof is a special marker used to identify the end of a string buffer input.
| eof           { TigerAST.EOF }
// The tokenize function will be converted into function which 
// has a return type of SqlParser.token
